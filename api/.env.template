# ===============================
# StudyBuddy Environment Variables
# ===============================
# IMPORTANT:
# Some variables are intentionally optional.
# Leaving certain fields EMPTY controls which LLM provider is used.
# Read comments carefully before filling values.

# ===============================
# Groq (Cloud LLM - Default Fallback)
# ===============================
# Used when no Ollama model is configured.
# Get an API key at: https://console.groq.com/keys

GROQ_API_KEY=your_groq_api_key_here

# Text-only chat model
GROQ_MODEL_NAME=llama-3.3-70b-versatile

# Vision-capable model (used for image-based inputs, if enabled)
GROQ_VISION_MODEL_NAME=meta-llama/llama-4-scout-17b-16e-instruct


# ===============================
# Vector Database / Retrieval
# ===============================
# These control how documents are chunked and retrieved

CHUNK_SIZE=1000        # Number of characters per chunk
CHUNK_OVERLAP=200      # Overlap between chunks
K=4                    # Number of chunks retrieved per query


# ===============================
# Ollama (Local LLM - Optional)
# ===============================
# If OLLAMA_MODEL_NAME is set, StudyBuddy will use Ollama instead of Groq.
# If OLLAMA_MODEL_NAME is EMPTY or commented out, Groq is used as a fallback.

# Example local text model:
# OLLAMA_MODEL_NAME=gemma3:1b

# Embedding model used for document indexing (can still be local even if chat uses Groq)
OLLAMA_EMBEDDING_MODEL_NAME=nomic-embed-text

# Optional local vision model
# If unset, vision requests fall back to Groq vision model
# OLLAMA_VISION_MODEL_NAME=gemma3:4b